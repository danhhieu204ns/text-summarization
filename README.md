# ğŸ“ Dá»± Ãn TÃ³m Táº¯t VÄƒn Báº£n Tiáº¿ng Viá»‡t (Vietnamese Text Summarization)

## Tá»•ng Quan

Dá»± Ã¡n nghiÃªn cá»©u vÃ  triá»ƒn khai hai phÆ°Æ¡ng phÃ¡p tÃ³m táº¯t vÄƒn báº£n tiáº¿ng Viá»‡t dá»±a trÃªn Deep Learning:
- **Extractive Summarization**: TrÃ­ch xuáº¥t cÃ¡c cÃ¢u quan trá»ng nháº¥t sá»­ dá»¥ng PhoBERT + MMR
- **Abstractive Summarization**: Táº¡o tÃ³m táº¯t má»›i báº±ng ViT5 vá»›i Parameter-Efficient Fine-Tuning (LoRA)

Dá»± Ã¡n bao gá»“m cÃ¡c Jupyter notebooks Ä‘á»ƒ huáº¥n luyá»‡n, Ä‘Ã¡nh giÃ¡ models, vÃ  má»™t web demo Ä‘Æ¡n giáº£n (FastAPI + React) Ä‘á»ƒ tráº£i nghiá»‡m káº¿t quáº£.

### Äáº·c Äiá»ƒm ChÃ­nh

- **Hai phÆ°Æ¡ng phÃ¡p AI**: Extractive (PhoBERT + MMR) vÃ  Abstractive (ViT5 + LoRA)
- **Tá»‘i Æ°u cho tiáº¿ng Viá»‡t**: Sá»­ dá»¥ng PhoBERT vÃ  ViT5 - cÃ¡c models cho tiáº¿ng Viá»‡t
- **Parameter-Efficient Fine-Tuning**: Sá»­ dá»¥ng LoRA thay vÃ¬ full fine-tuning Ä‘á»ƒ tiáº¿t kiá»‡m tÃ i nguyÃªn
- **Notebooks chi tiáº¿t**: Tá»« data preprocessing, training, Ä‘áº¿n evaluation
- **Web demo**: Giao diá»‡n Ä‘Æ¡n giáº£n Ä‘á»ƒ test models

## Kiáº¿n TrÃºc AI

```
INPUT TEXT (Vietnamese)
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                               â”‚
    â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTRACTIVE     â”‚         â”‚  ABSTRACTIVE    â”‚
â”‚  PhoBERT-base   â”‚         â”‚  ViT5-base      â”‚
â”‚  + MMR          â”‚         â”‚  + LoRA         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                               â”‚
    â”‚ Sentence                      â”‚ Seq2Seq
    â”‚ Selection                     â”‚ Generation
    â”‚                               â”‚
    â–¼                               â–¼
Selected Sentences          Generated Summary
```

## CÃ´ng Nghá»‡ vÃ  Models

### Core AI Components
- **PyTorch 2.1+**: Deep learning framework
- **Transformers (Hugging Face)**: Pre-trained models vÃ  tokenizers
- **PEFT (LoRA)**: Parameter-Efficient Fine-Tuning - giáº£m parameters cáº§n train 99%
- **Underthesea**: Vietnamese NLP toolkit cho tokenization

### Pre-trained Models

#### 1. PhoBERT-base (vinai/phobert-base)
- **Kiáº¿n trÃºc**: RoBERTa-base adapted for Vietnamese
- **Parameters**: 135M
- **Pre-training**: 20GB Vietnamese text (Wikipedia, news, social media)
- **Sá»­ dá»¥ng**: Táº¡o sentence embeddings cho extractive summarization

#### 2. ViT5-base (VietAI/vit5-base)
- **Kiáº¿n trÃºc**: T5 (Text-to-Text Transfer Transformer) for Vietnamese
- **Parameters**: 250M (base model) + ~0.6M (LoRA adapter)
- **Pre-training**: Vietnamese Wikipedia, news, books
- **Fine-tuning**: Sá»­ dá»¥ng LoRA trÃªn dataset tÃ³m táº¯t tin tá»©c tiáº¿ng Viá»‡t

### Algorithms

#### Maximal Marginal Relevance (MMR)
- **CÃ´ng thá»©c**: `MMR = Î» Ã— Sim(Si, D) - (1-Î») Ã— max Sim(Si, Sj)`
- **Má»¥c Ä‘Ã­ch**: Balance giá»¯a relevance vÃ  diversity
- **Tham sá»‘ Î»**: Äiá»u chá»‰nh trade-off (Î»=0.7 trong dá»± Ã¡n)

### Web Demo (FastAPI + React)
Má»™t giao diá»‡n Ä‘Æ¡n giáº£n Ä‘á»ƒ test cÃ¡c models Ä‘Ã£ huáº¥n luyá»‡n:
- **Backend**: FastAPI phá»¥c vá»¥ inference API
- **Frontend**: React UI Ä‘á»ƒ input/output
- **Má»¥c Ä‘Ã­ch**: Demo vÃ  validation

## YÃªu Cáº§u Há»‡ Thá»‘ng

### Pháº§n Má»m
- Python 3.8+ (khuyáº¿n nghá»‹ 3.10)
- Jupyter Notebook / JupyterLab
- CUDA toolkit 11.8+ (cho GPU training/inference)

### Pháº§n Cá»©ng
- **RAM**: Tá»‘i thiá»ƒu 16GB (khuyáº¿n nghá»‹ 32GB cho training)
- **GPU**: NVIDIA GPU vá»›i Ã­t nháº¥t 8GB VRAM (khuyáº¿n nghá»‹ RTX 3060 trá»Ÿ lÃªn)
- **á»” Ä‘Ä©a**: 10GB+ cho models, datasets, vÃ  checkpoints

### Cho Web Demo (tÃ¹y chá»n)
- Node.js 16+ vÃ  npm 8+ (náº¿u muá»‘n cháº¡y React frontend)
- RAM 8GB+ lÃ  Ä‘á»§ cho inference

## HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. Setup MÃ´i TrÆ°á»ng

```powershell
# Clone dá»± Ã¡n
git clone <repository-url>
cd nlp

# Táº¡o mÃ´i trÆ°á»ng áº£o Python
python -m venv venv
.\venv\Scripts\Activate.ps1

# CÃ i Ä‘áº·t dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers peft underthesea scikit-learn numpy pandas jupyter
```

### 2. KhÃ¡m PhÃ¡ Notebooks

#### `extractive-summarization.ipynb`
Notebook thá»­ nghiá»‡m extractive summarization:
- Load PhoBERT vÃ  táº¡o sentence embeddings
- Implement vÃ  so sÃ¡nh cÃ¡c thuáº­t toÃ¡n: MMR, TextRank, centroid-based
- ÄÃ¡nh giÃ¡ káº¿t quáº£ trÃªn vÄƒn báº£n máº«u
- Visualization similarity matrix

#### `abstractive-summarization.ipynb`
Notebook huáº¥n luyá»‡n abstractive model:
- Load vÃ  preprocess dataset tÃ³m táº¯t tin tá»©c
- Fine-tune ViT5 vá»›i LoRA adapter
- Training loop vá»›i validation
- Evaluation metrics (ROUGE scores)
- Generate vÃ  so sÃ¡nh káº¿t quáº£
- Save checkpoint Ä‘á»ƒ deploy

### 3. Cháº¡y Web Demo (TÃ¹y Chá»n)

Náº¿u muá»‘n test qua giao diá»‡n web:

```powershell
# Backend
cd summarization-demo\backend
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
uvicorn app:app --reload

# Frontend (terminal má»›i)
cd summarization-demo\frontend
npm install
npm run dev
```

Truy cáº­p http://localhost:5173 Ä‘á»ƒ test models.

## API Reference (Web Demo)

Web demo cung cáº¥p REST API Ä‘Æ¡n giáº£n Ä‘á»ƒ inference:

### POST `/summarize`

**Request:**
```json
{
  "text": "VÄƒn báº£n cáº§n tÃ³m táº¯t...",
  "mode": "both",
  "top_k": 3
}
```

**Response:**
```json
{
  "extractive": "CÃ¢u 1. CÃ¢u 2. CÃ¢u 3.",
  "abstractive": "TÃ³m táº¯t Ä‘Æ°á»£c generate..."
}
```

Chi tiáº¿t xem [INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md).

## Cáº¥u TrÃºc Dá»± Ãn

```
nlp/
â”œâ”€â”€ abstractive-summarization.ipynb    # [CORE] Notebook train ViT5 + LoRA
â”‚       â”œâ”€â”€ Data loading & preprocessing
â”‚       â”œâ”€â”€ LoRA configuration
â”‚       â”œâ”€â”€ Training loop vá»›i validation
â”‚       â”œâ”€â”€ Evaluation (ROUGE scores)
â”‚       â””â”€â”€ Save checkpoint
â”‚
â”œâ”€â”€ extractive-summarization.ipynb     # [CORE] Notebook thá»­ nghiá»‡m extractive
â”‚       â”œâ”€â”€ PhoBERT embedding
â”‚       â”œâ”€â”€ MMR algorithm implementation
â”‚       â”œâ”€â”€ Comparison vá»›i TextRank, centroid
â”‚       â””â”€â”€ Visualization
â”‚
â”œâ”€â”€ documents.txt                       # Sample data Ä‘á»ƒ test
â”œâ”€â”€ INSTALLATION_GUIDE.md               # HÆ°á»›ng dáº«n setup chi tiáº¿t
â”œâ”€â”€ README.md                           # File nÃ y
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ t5_new_lora/
â”‚       â””â”€â”€ checkpoint-1865/               # [CORE] LoRA adapter trained
â”‚           â”œâ”€â”€ adapter_config.json        # LoRA hyperparameters
â”‚           â”œâ”€â”€ adapter_model.safetensors  # Trained weights (~2.5MB)
â”‚           â”œâ”€â”€ trainer_state.json         # Training history
â”‚           â””â”€â”€ tokenizer files...
â”‚
â””â”€â”€ summarization-demo/                 # [DEMO] Web application
    â”œâ”€â”€ backend/                        # FastAPI server
    â”‚   â”œâ”€â”€ app.py                      # Load models + API endpoints
    â”‚   â””â”€â”€ requirements.txt
    â””â”€â”€ frontend/                       # React UI
        â”œâ”€â”€ src/App.jsx                 # Simple UI
        â””â”€â”€ package.json
```

## PhÆ°Æ¡ng PhÃ¡p AI

### 1. Extractive Summarization: PhoBERT + MMR

#### Kiáº¿n TrÃºc vÃ  Quy TrÃ¬nh

```
Input Text
    â”‚
    â”œâ”€â–º Sentence Tokenization (Underthesea)
    â”‚
    â”œâ”€â–º PhoBERT Encoding
    â”‚   â””â”€â–º [CLS] token embedding cho má»—i cÃ¢u
    â”‚       â””â”€â–º 768-dim vectors
    â”‚
    â”œâ”€â–º Compute Similarities
    â”‚   â”œâ”€â–º Sentence â†” Document (relevance)
    â”‚   â””â”€â–º Sentence â†” Selected Sentences (diversity)
    â”‚
    â”œâ”€â–º MMR Selection Algorithm
    â”‚   â””â”€â–º Iteratively chá»n cÃ¢u tá»‘i Æ°u
    â”‚       MMR = Î»Â·Sim(Si,D) - (1-Î»)Â·max(Sim(Si,Sj))
    â”‚
    â””â”€â–º Output: Top-k sentences (sorted by position)
```

### 2. Abstractive Summarization: ViT5 + LoRA

#### Kiáº¿n TrÃºc vÃ  Quy TrÃ¬nh

```
Input Text
    â”‚
    â”œâ”€â–º Preprocessing: "summarize: " + text
    â”‚
    â”œâ”€â–º ViT5 Tokenizer
    â”‚   â””â”€â–º Input IDs [batch, seq_len]
    â”‚
    â”œâ”€â–º ViT5 Encoder (frozen base weights)
    â”‚   â””â”€â–º Contextualized embeddings
    â”‚
    â”œâ”€â–º LoRA Adapter Layers (trainable)
    â”‚   â””â”€â–º Low-rank matrices: A [rÃ—d], B [dÃ—r]
    â”‚   â””â”€â–º Updated weights: W' = W + BÂ·A
    â”‚
    â”œâ”€â–º ViT5 Decoder (frozen base + LoRA)
    â”‚   â””â”€â–º Autoregressive generation
    â”‚
    â””â”€â–º Output: Generated summary
```

#### Parameter-Efficient Fine-Tuning vá»›i LoRA

**Ã tÆ°á»Ÿng chÃ­nh:**
- Full fine-tuning: Update táº¥t cáº£ 250M parameters
- LoRA: Chá»‰ train ~0.6M parameters (0.24%) báº±ng cÃ¡ch:
  - Freeze táº¥t cáº£ weights cá»§a base model
  - ThÃªm low-rank adapter matrices vÃ o attention layers

**LoRA Math:**
```
Original attention: h = Wâ‚€Â·x
LoRA update: h = Wâ‚€Â·x + Î”WÂ·x
           where Î”W = BÂ·A
           
Wâ‚€: [dÃ—d] frozen weights
A: [rÃ—d] trainable (down-projection)  
B: [dÃ—r] trainable (up-projection)
r: rank (8 hoáº·c 16) << d (768)

Parameters to train: 2Â·rÂ·d thay vÃ¬ dÂ²
```

## Cáº¥u HÃ¬nh vÃ  TÃ¹y Chá»‰nh

### Extractive Hyperparameters

```python
# Trong notebook hoáº·c app.py

# MMR parameters
lambda_ = 0.7              # Balance: 0.7 relevance + 0.3 diversity
                           # TÄƒng Î» â†’ more relevant, less diverse
                           # Giáº£m Î» â†’ more diverse, less relevant

top_k = 3                  # Sá»‘ cÃ¢u trong tÃ³m táº¯t
                           # Auto: dá»±a vÃ o avg sentence length
                           # Manual: user specify

# PhoBERT encoding
max_length = 256           # Max tokens per sentence
                           # CÃ¢u dÃ i hÆ¡n sáº½ bá»‹ truncate
```

## Performance vÃ  Benchmarks

### Quality Metrics (trÃªn test set)

#### Abstractive (ViT5 + LoRA)
- **ROUGE-1**: 53.72
- **ROUGE-2**: 26.23  
- **ROUGE-L**: 36.17
- **ROUGE-Lsum**: 36.20
- **BLEU (sacreBLEU)**: 8.45
- **BERTScore**:
  - Precision: 0.70
  - Recall: 0.70
  - F1: 0.70

#### Extractive (PhoBERT + MMR)
- **ROUGE-1**: 37.79
- **ROUGE-2**: 18.25
- **ROUGE-L**: 24.70
- **ROUGE-Lsum**: 24.83
- **BLEU**: 3.65
- **BERTScore**:
  - Precision: 0.65
  - Recall: 0.73
  - F1: 0.69

*ROUGE scores cÃ ng cao (max=100.0) cÃ ng giá»‘ng reference summary*
*BERTScore Ä‘o semantic similarity, F1 cÃ ng cao cÃ ng tá»‘t (max=1.0)*


## References vÃ  Papers

### Foundational Papers

1. **BERT & RoBERTa**
   - Devlin et al. (2018): "BERT: Pre-training of Deep Bidirectional Transformers"
   - Liu et al. (2019): "RoBERTa: A Robustly Optimized BERT Pretraining Approach"

2. **T5 Architecture**
   - Raffel et al. (2020): "Exploring the Limits of Transfer Learning with T5"
   - Text-to-Text framework cho NLP tasks

3. **LoRA (Low-Rank Adaptation)**
   - Hu et al. (2021): "LoRA: Low-Rank Adaptation of Large Language Models"
   - ICLR 2022, Microsoft Research
   - Key insight: `Î”W = BÂ·A` vá»›i rank r << d

4. **MMR (Maximal Marginal Relevance)**
   - Carbonell & Goldstein (1998): "The Use of MMR for Text Summarization"
   - Balance relevance vÃ  diversity

### Vietnamese NLP Models

1. **PhoBERT**
   - Nguyen & Nguyen (2020): "PhoBERT: Pre-trained language models for Vietnamese"
   - VinAI Research
   - Paper: https://arxiv.org/abs/2003.00744

2. **ViT5**
   - VietAI Team (2021): "ViT5: Pretrained Text-to-Text Transformer for Vietnamese"
   - Based on T5 architecture
   - GitHub: https://github.com/vietai/ViT5

### Datasets

- OpenHust/vietnamese-summarization 


**â­ Náº¿u dá»± Ã¡n há»¯u Ã­ch cho research/learning, hÃ£y cho má»™t star! â­**
